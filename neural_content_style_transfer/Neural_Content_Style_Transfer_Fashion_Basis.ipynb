{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')","metadata":{"id":"lVjiOTKcbxV9","outputId":"045a7119-e464-47d8-b66c-d6caad7a0a96","executionInfo":{"status":"ok","timestamp":1654161147265,"user_tz":-120,"elapsed":21015,"user":{"displayName":"Daniel Puhl","userId":"10022481371732645414"}},"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:53:59.641454Z","iopub.execute_input":"2022-06-02T14:53:59.641805Z","iopub.status.idle":"2022-06-02T14:53:59.645723Z","shell.execute_reply.started":"2022-06-02T14:53:59.641776Z","shell.execute_reply":"2022-06-02T14:53:59.644899Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/artxfashion/data/\"","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:53:59.648083Z","iopub.execute_input":"2022-06-02T14:53:59.649260Z","iopub.status.idle":"2022-06-02T14:53:59.655754Z","shell.execute_reply.started":"2022-06-02T14:53:59.649220Z","shell.execute_reply":"2022-06-02T14:53:59.654765Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Neural Content Style Transfer\n\n\nBasiert auf (1) https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf and (2) https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer.\n\nIn diesem Notebook wird der klassische Neural Style Transfer (NST) zu Neural Content Style Transfer (NCST) erweitert.\n\n\n\n\n\n\n","metadata":{"id":"vMj4eOqqbq49","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# import resources\n%matplotlib inline\n\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nimport torch\nimport torch.optim as optim\nimport requests\nfrom torchvision import transforms, models\nimport torch.nn","metadata":{"id":"jvYwdTTFbq5A","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:53:59.657779Z","iopub.execute_input":"2022-06-02T14:53:59.658696Z","iopub.status.idle":"2022-06-02T14:53:59.668570Z","shell.execute_reply.started":"2022-06-02T14:53:59.658642Z","shell.execute_reply":"2022-06-02T14:53:59.667579Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Load in the pre-trained VGG\n\nDie Feautre Maps des vortrainierten 19-schichtigen VGG Netzwerks werden betrachtet. Im unteren Bild sieht man, dass die Convolution Schichten nach ihrem Stack und ihrer Reinfolge in dem Stack benannt sind. Conv_5_4 bedeutet in dem 5ten Stack die 4te Schicht und damit die letzte Convolution Schicht.\n\n<img src='https://github.com/danielpuhl/deep-learning-v2-pytorch/blob/master/style-transfer/notebook_ims/vgg19_convlayers.png?raw=1' width=80% />\n","metadata":{"id":"Bg4nguxUiQkb","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# laden der Feature Extraktion Schichten des VGG19, den Klassifizier Part benötigen wir nicht\nvgg = models.vgg19(pretrained=True).features\n\n# alle VGG Parameter werden eingefroren, da wir nicht trainieren wollen\nfor param in vgg.parameters():\n    param.requires_grad_(False)","metadata":{"id":"sik2eSDJbq5C","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:53:59.670240Z","iopub.execute_input":"2022-06-02T14:53:59.670867Z","iopub.status.idle":"2022-06-02T14:54:01.602520Z","shell.execute_reply.started":"2022-06-02T14:53:59.670826Z","shell.execute_reply":"2022-06-02T14:54:01.601141Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Model auf die GPU umziehen\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvgg.to(device)","metadata":{"id":"st-73HPebq5C","outputId":"32f84ddc-4e22-44c7-9184-24bc79894dea","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:01.611806Z","iopub.execute_input":"2022-06-02T14:54:01.616568Z","iopub.status.idle":"2022-06-02T14:54:01.669002Z","shell.execute_reply.started":"2022-06-02T14:54:01.616520Z","shell.execute_reply":"2022-06-02T14:54:01.668041Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#Laden des Fashion Bildes und des Art Bildes\n\nMit Hilfe des load_image Funktion können Bilder beliebiger Größe geladen werden.\nÜbernommen aus (2)","metadata":{"id":"Iua8qzLPbq5D","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def load_image(img_path, max_size=400, shape=None):\n    ''' Laden und transformieren von Bildern und Sicherstellung dass das Bild <= 400 pixels in der x-y dimension.'''\n    if \"http\" in img_path:\n        response = requests.get(img_path)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(img_path).convert('RGB')\n    \n    # große Bilder beeinträchtigen die Ausführungszeit\n    if max(image.size) > max_size:\n        size = max_size\n    else:\n        size = max(image.size)\n    \n    if shape is not None:\n        size = shape\n    \n    in_transform = transforms.Compose([\n      transforms.Resize(size),\n      transforms.ToTensor(),\n      transforms.Normalize((0.485, 0.456, 0.406), \n      (0.229, 0.224, 0.225))])                        \n\n\n    # entfernen des transparenten alpha channels (:3) und hinzufügen der batch dimension\n    image = in_transform(image)[:3,:,:].unsqueeze(0)\n    \n    return image","metadata":{"id":"VRApHiL1bq5D","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:01.670610Z","iopub.execute_input":"2022-06-02T14:54:01.671341Z","iopub.status.idle":"2022-06-02T14:54:01.688535Z","shell.execute_reply.started":"2022-06-02T14:54:01.671295Z","shell.execute_reply":"2022-06-02T14:54:01.687490Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# laden des fashion und des art bildes und selbe Größe setzen\n# das fashion bild ist hier der Einfachheit halber ein weißer Canvas bzw. ein weißer Canvas mit den Kontouren des Art Bildes\nfashion_image = load_image(os.path.join(DATA_PATH,\"images_fashion/Katze_scetch3.png\")).to(device)\nart_image = load_image(os.path.join(DATA_PATH,\"images_art/Katze.png\"), shape=fashion_image.shape[-2:]).to(device)\n","metadata":{"id":"fbz23AQBbq5E","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:01.691068Z","iopub.execute_input":"2022-06-02T14:54:01.692618Z","iopub.status.idle":"2022-06-02T14:54:01.852123Z","shell.execute_reply.started":"2022-06-02T14:54:01.692568Z","shell.execute_reply":"2022-06-02T14:54:01.851025Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\n# Hilfsfunktion um ein Tensor Bild wieder darzustellen (un-normalizing, konvertieren des Tensors in ein NumPy Bild), entnommen aus (2)\ndef im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1,2,0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n    image = image.clip(0, 1)\n\n    return image","metadata":{"id":"W-JBcx9vbq5E","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:01.853706Z","iopub.execute_input":"2022-06-02T14:54:01.854141Z","iopub.status.idle":"2022-06-02T14:54:01.867397Z","shell.execute_reply.started":"2022-06-02T14:54:01.854100Z","shell.execute_reply":"2022-06-02T14:54:01.865817Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Bilder anzeigen\n# fashion Bild und art Bild nebeneinander\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))\n\n# content and style ims side-by-side\nax1.imshow(im_convert(fashion_image))\nax2.imshow(im_convert(art_image))","metadata":{"id":"R1rlZKqEmyXx","outputId":"f36d2ea6-9a34-40b7-8e9b-b9ab0097f76a","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:01.869157Z","iopub.execute_input":"2022-06-02T14:54:01.869758Z","iopub.status.idle":"2022-06-02T14:54:02.800704Z","shell.execute_reply.started":"2022-06-02T14:54:01.869716Z","shell.execute_reply":"2022-06-02T14:54:02.799837Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Festlegen","metadata":{"id":"2BxVXPQpS9x7","pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"\n## 1. Auswahl und Gewichtung der Schichten für die Stlye Repräsentation\nAuswahl:\n- Layer die in Paper (1) betrachtet werden: 'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv4_2', 'conv5_1'\n- Layer 'Conv4_2' wird wie im Paper als content layer gewählt. Im Paper wird diese für die Berechnung des Style losses entfernt, wir behalten sie drin um noch mehr den Content des Art Bildes rauszustellen\n- Erweitern der betrachteten Layer führt schnell zu einem Memory Overflow ca. 14 GB\n\nGewichtung:\n- Für jede der Schichten für die Berechnung des Style Losses können Gewichte festgelegt werden\n- Hohe Gewichtung der früheren Schichten sorgt für größere Style Artefakte, Hohe Gewichtung der späteren Schichten sorgt für kleinere Style Artefakte\n\n\n","metadata":{"id":"CfOmcqrobq5H","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Auswahl der Schichten und der Gewichte\n# Content Layer mal rausgenommen 'conv4_2':0.2,\nstyle_layers_and_style_weights = {'conv1_1':1.,'conv2_1':0.75, 'conv3_1':0.2, 'conv4_1':0.2,'conv5_1':0.2}\n\n# Auwahl der Content Layer\ncontent_layer = 'conv4_2'\n\nstyle_layers = list(style_layers_and_style_weights.keys())\nselected_layers = style_layers.__add__([content_layer])","metadata":{"id":"DbEPB3_8Awjl","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.802229Z","iopub.execute_input":"2022-06-02T14:54:02.802886Z","iopub.status.idle":"2022-06-02T14:54:02.809730Z","shell.execute_reply.started":"2022-06-02T14:54:02.802829Z","shell.execute_reply":"2022-06-02T14:54:02.808635Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## 2. Gewichtung des Verhältnisses zwischen Style und Content auf dem tranformierten Fashion Bild\n\nAuf Basis des Paper (1) und angepasst an unseren Neural Content Style Ansatz definieren wir Alphas (`content_fashion_weight` und 'content_art_weight') und ein Beta (`style_art_weight`). Dieses Verhältnis beeinflusst, wie _stylisiert_ Ihr endgültiges Bild ist. Es wird empfohlen, den Wert für \"content_fashion_\nweight\" = 1 zu belassen und \"style_weight\" so einzustellen, dass das gewünschte Verhältnis erreicht wird.","metadata":{"id":"9WM0il5ZT5FU","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Festlegung der alphas und des beta\ncontent_fashion_weight = 0.5  # alpha_fashion\ncontent_art_weight = 10 #alpha_art\nstyle_art_weight = 1e6  # beta","metadata":{"id":"X-GuLD6xbq5H","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.812032Z","iopub.execute_input":"2022-06-02T14:54:02.812450Z","iopub.status.idle":"2022-06-02T14:54:02.821119Z","shell.execute_reply.started":"2022-06-02T14:54:02.812413Z","shell.execute_reply":"2022-06-02T14:54:02.820190Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## 3. Anzahl der Optimierungs-/Iterationsschritte auf dem transformed_fashion_image\nMindestens 2000 nach Gefühl, umso mehr umso besser","metadata":{"id":"cvoB0hPHV0T5","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"#Festlegen der Optimierungsschritte\nsteps = 5000 ","metadata":{"id":"NUdj7p0dVz5C","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.823554Z","iopub.execute_input":"2022-06-02T14:54:02.824231Z","iopub.status.idle":"2022-06-02T14:54:02.830828Z","shell.execute_reply.started":"2022-06-02T14:54:02.824168Z","shell.execute_reply":"2022-06-02T14:54:02.829609Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## 4. Learning Rate des Optimizers\n","metadata":{"id":"YEQq8fEoW2F2","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"learning_rate = 0.003","metadata":{"id":"VEtYlrEZW1Gw","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.836196Z","iopub.execute_input":"2022-06-02T14:54:02.836856Z","iopub.status.idle":"2022-06-02T14:54:02.844610Z","shell.execute_reply.started":"2022-06-02T14:54:02.836815Z","shell.execute_reply":"2022-06-02T14:54:02.843499Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Content und Style Features\n","metadata":{"id":"_UtldpHqbq5F","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Convolutional Layer und ihr Name in der VGG Definition\ndict_layers_name_to_num = {'conv1_1':'0',\n                'conv1_2':'2',\n                'conv2_1':'5', \n                'conv2_2':'7', \n                'conv3_1':'10', \n                'conv3_2':'12',\n                'conv3_3':'14',\n                'conv3_4':'16',\n                'conv4_1':'19',\n                'conv4_2':'21',\n                'conv4_3':'23',\n                'conv4_4':'25',\n                'conv5_1':'28',\n                'conv5_2':'30',\n                'conv5_3':'32',\n                'conv5_4':'34'}\ndict_layers_num_to_name = {v: k for k, v in dict_layers_name_to_num.items()}\n\nselected_layers_num = [dict_layers_name_to_num[layer] for layer in selected_layers]","metadata":{"id":"KAw47PLZaSOF","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.846612Z","iopub.execute_input":"2022-06-02T14:54:02.847626Z","iopub.status.idle":"2022-06-02T14:54:02.859075Z","shell.execute_reply.started":"2022-06-02T14:54:02.847519Z","shell.execute_reply":"2022-06-02T14:54:02.857297Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Methode um die Feature Maps einer spezifizierten Schicht auszugegeben\n# basiert auf(2)\n# ggf. ein paar Schichten rauslassen um den GPU Speicher nicht auszureizen\n\n\ndef get_features(image, model, selected_layers):\n    \"\"\" Run an image forward through a model and get the features for \n        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n    \"\"\"             \n         \n        \n    features = {}\n    x = image\n\n    # model._modules ist ein dictionary in dem jede Schicht des Models gelistet ist\n    for name, layer in model._modules.items():\n        x = layer(x)\n        if name in selected_layers_num:\n            features[dict_layers_num_to_name[name]] = x\n        \n            \n    return features","metadata":{"id":"LuctRmmObq5F","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.861516Z","iopub.execute_input":"2022-06-02T14:54:02.862108Z","iopub.status.idle":"2022-06-02T14:54:02.874038Z","shell.execute_reply.started":"2022-06-02T14:54:02.862062Z","shell.execute_reply":"2022-06-02T14:54:02.872704Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"WjC1LVzS0zHr","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"cxstxXXmyjoV","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iqIjpIMzw7g3","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"cq-eOyQ-vlDb","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Gram Matrix (siehe (2))\n\n\nThe output of every convolutional layer is a Tensor with dimensions associated with the `batch_size`, a depth, `d` and some height and width (`h`, `w`). The Gram matrix of a convolutional layer can be calculated as follows:\n* Get the depth, height, and width of a tensor using `batch_size, d, h, w = tensor.size`\n* Reshape that tensor so that the spatial dimensions are flattened\n* Calculate the gram matrix by multiplying the reshaped tensor by it's transpose \n\n","metadata":{"id":"gUCapIlVbq5F","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"#Funktion zur Berechnung der Gram Matrix aus (2) entnommen\ndef gram_matrix(tensor):\n    \"\"\" Calculate the Gram Matrix of a given tensor \n        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n    \"\"\"\n    \n    # get the batch_size, depth, height, and width of the Tensor\n    b, d, h, w = tensor.size()\n    \n    # reshape so we're multiplying the features for each channel\n    tensor = tensor.view(b * d, h * w)\n    \n    # calculate the gram matrix\n    gram = torch.mm(tensor, tensor.t())\n    \n    return gram ","metadata":{"id":"xofLULXmbq5G","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.876467Z","iopub.execute_input":"2022-06-02T14:54:02.877042Z","iopub.status.idle":"2022-06-02T14:54:02.884479Z","shell.execute_reply.started":"2022-06-02T14:54:02.876998Z","shell.execute_reply":"2022-06-02T14:54:02.883325Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Feautres des Art und des Fashion Bildes extrahieren","metadata":{"id":"GdEltlYLy7DV","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Feature Maps des Art und des Fashion Bildes speichern\nart_image_features = get_features(art_image, vgg, selected_layers)\nfashion_image_features = get_features(fashion_image, vgg, selected_layers)\n\n\n# Gram Matrizen für jede Schicht bei Input des Art Bildes berechnen\nstyle_grams = {layer: gram_matrix(art_image_features[layer]) for layer in art_image_features}\n\n# Erstellung unseres target transformed_fashion_image welches iterativ basierend auf dem fashion_image transformiert wird\ntransformed_fashion_image = fashion_image.clone().requires_grad_(True).to(device)","metadata":{"id":"pBxAoF3abq5G","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.886391Z","iopub.execute_input":"2022-06-02T14:54:02.887342Z","iopub.status.idle":"2022-06-02T14:54:02.903563Z","shell.execute_reply.started":"2022-06-02T14:54:02.887300Z","shell.execute_reply":"2022-06-02T14:54:02.902381Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Durchfürung der Optimierung\n","metadata":{"id":"_5aEGi9LX2Hq","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# transformed_fashion_image alle \"show_every\" Schritte anzeigen\nshow_every = 100\n\n\noptimizer = optim.Adam([transformed_fashion_image], lr=learning_rate)\n\n\nfor ii in range(1, steps+1):\n    \n    \n    transformed_fashion_image_features = get_features(transformed_fashion_image, vgg, selected_layers)\n    \n    #content_fashion loss\n    content_fashion_loss = torch.mean((transformed_fashion_image_features[content_layer] - fashion_image_features[content_layer])**2)\n\n\n    # content_art loss\n    content_art_loss = torch.mean((transformed_fashion_image_features[content_layer] - art_image_features[content_layer])**2)\n    \n    # style loss\n    # mit 0 initialisieren\n    style_loss = 0\n    # für jede Schicht den loss der jeweiligen Gram Matrix daraufrechnen\n    for layer in style_layers_and_style_weights:\n  \n        transformed_fashion_image_feature = transformed_fashion_image_features[layer]\n        transformed_fashion_image_gram = gram_matrix(transformed_fashion_image_feature)\n        _, d, h, w = transformed_fashion_image.shape\n        \n        style_gram = style_grams[layer]\n        \n        layer_style_loss = style_layers_and_style_weights[layer] * torch.mean(( transformed_fashion_image_gram - style_gram)**2)\n        \n        style_loss += layer_style_loss / (d * h * w)\n        \n    \n    total_loss = content_art_weight * content_art_loss + style_art_weight * style_loss + content_fashion_weight * content_fashion_loss\n    \n    # updaten des transformed_fashion_image\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    \n    if  ii % show_every == 0:\n        print('Total loss: ', total_loss.item())\n        plt.imshow(im_convert(transformed_fashion_image))\n        plt.show()","metadata":{"id":"DLYg3lR0bq5H","scrolled":true,"outputId":"243f0f28-dc6f-4cdf-d37f-fa5a809cab3c","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-06-02T14:54:02.905316Z","iopub.execute_input":"2022-06-02T14:54:02.905778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"XKyS-PgId_NK","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display the Target Image","metadata":{"id":"avQ4OJjTbq5I","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# display fashion image canvas and transformed fashion image\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\nax1.imshow(im_convert(fashion_image))\nax2.imshow(im_convert(transformed_fashion_image))","metadata":{"id":"4f5aDoNRbq5I","pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]}]}